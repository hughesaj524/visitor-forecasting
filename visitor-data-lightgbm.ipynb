{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(524)\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder#, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The factor of the dataset to use for training vs \n",
    "TRAIN_SCALE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'tra': pd.read_csv('data/air/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('data/air/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('data/hpg/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('data/air/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('data/hpg/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('data/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('sample_submission.csv'),\n",
    "    'hol': pd.read_csv('data/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "}\n",
    "\n",
    "# Merge store id relation data onto HPG data\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitting up datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    data[df] = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### engineering visitor statistics on a store by store basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding restaurant data and holiday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id'])\n",
    "lbl = LabelEncoder()\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date'])\n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### setting up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(data['tra'], stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(data['tes'], stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "# deal with missing data\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, pred):\n",
    "    return mean_squared_error(y, pred) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = train.sort_values('visit_date')\n",
    "values = (train['visitors'].values).reshape(-1,1)\n",
    "values = values.astype('float32')\n",
    "\"\"\"\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "\"\"\"\n",
    "scaled = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176475 75633\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(scaled) * 0.7)\n",
    "test_size = len(scaled) - train_size\n",
    "\n",
    "train = train.drop([\"air_store_id\", \"visit_date\", \"visitors\"], axis=1)\n",
    "\n",
    "X_train, X_test = train.iloc[0:train_size,:], train.iloc[train_size:len(train),:]\n",
    "y_train, y_test = scaled[0:train_size,:].ravel(), scaled[train_size:len(scaled),:].ravel()\n",
    "print(len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1296 candidates, totalling 6480 fits\n"
     ]
    }
   ],
   "source": [
    "tsplit = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "model = lgb.LGBMModel(\n",
    "    boosting = \"gbdt\",\n",
    "    objective = \"regression\",\n",
    "    learning_rate = 0.003,\n",
    "    num_leaves = 32,\n",
    "    n_estimators = 39000,\n",
    "    min_child_weight = 1e-2, #9e-4,\n",
    "    min_split_gain = 0,\n",
    "#    bagging_freq = 1,\n",
    "    bagging_fraction = 0.8,\n",
    "    feature_fraction = 0.8,\n",
    "    verbose = 100\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1],\n",
    "    'n_estimators': [13000,15000,17000],\n",
    "    'min_child_weight': [1e-3, 1e-2, 1e-1],\n",
    "    'num_leaves': [20,25,30,35],\n",
    "    'bagging_freq': [0,0.2,0.4,0.6,0.8,1]\n",
    "}\n",
    "\n",
    "gbm = GridSearchCV(model, param_grid, n_jobs=-1, cv=tsplit, scoring=\"neg_mean_squared_error\", verbose=2)\n",
    "gbm.fit(X_train, y_train, eval_metric=\"rmse\", verbose=20)\n",
    "print(\"Best params:\", gbm.best_params_)\n",
    "\n",
    "#model.fit(X_train, y_train, eval_metric=\"rmse\", verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)#.reshape((-1, 1))\n",
    "\n",
    "#X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[2]))\n",
    "## Invert scaling for forecast\n",
    "#inv_yhat = np.concatenate((yhat, X_test.iloc[:, 1:]), axis=1)\n",
    "#inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "#inv_yhat = inv_yhat[:,0]\n",
    "## Invert scaling for actual\n",
    "#y_test = y_test.reshape((len(y_test), 1))\n",
    "#inv_y = np.concatenate((y_test, X_test.iloc[:, 1:]), axis=1)\n",
    "#inv_y = scaler.inverse_transform(inv_y)\n",
    "#inv_y = (inv_y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lgbm_rmse = rmse(y_test, yhat)\n",
    "print('Test RMSE: %.3f' % lgbm_rmse)\n",
    "plt.plot((y_test), (yhat), \"m,\")\n",
    "plt.xlabel(\"visit data\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.axis(\"equal\")\n",
    "plt.axis([0,150,-25,100])\n",
    "print(max(y_test) - max(yhat))\n",
    "\n",
    "# plotting identity function\n",
    "lims = plt.axis()\n",
    "plt.plot(lims, lims, '0.5', alpha=0.75, zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(y%1 for y in y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(set(test.columns) - {\"id\", \"visitors\", \"air_store_id\", \"visit_date\"})\n",
    "test_yhat = model.predict(test[x])#.reshape((-1, 1))\n",
    "\n",
    "# X_test = X_test.values.reshape((X_test.shape[0], X_test.shape[2]))\n",
    "# Invert scaling for forecast\n",
    "#test_inv_yhat = np.concatenate((test_yhat, test[x].iloc[:, 1:]), axis=1)\n",
    "#test_inv_yhat = scaler.inverse_transform(test_inv_yhat)\n",
    "#test_inv_yhat = np.expm1(test_inv_yhat[:,0])\n",
    "\n",
    "test[\"visitors\"] = test_yhat\n",
    "\n",
    "test[[\"id\", \"visitors\"]].to_csv(\"LGB_CV_sub.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### todo:\n",
    "\n",
    "- try removing outliers (visits > 300?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
