{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from operator import xor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_metrics import rmsle\n",
    "%matplotlib inline\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "air = {\n",
    "    \"reserve\": pd.read_csv(\"data/air/air_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/air/air_store_info.csv\"),\n",
    "    \"visit_data\": pd.read_csv(\"data/air/air_visit_data.csv\", parse_dates=[\"visit_date\"])\n",
    "}\n",
    "\n",
    "hpg = {\n",
    "    \"reserve\": pd.read_csv(\"data/hpg/hpg_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/hpg/hpg_store_info.csv\")\n",
    "}\n",
    "\n",
    "date_info = pd.read_csv(\"data/date_info.csv\", parse_dates=[\"calendar_date\"])\n",
    "store_id_relation = pd.read_csv(\"data/store_id_relation.csv\")\n",
    "\n",
    "def remove_outliers(data):\n",
    "    df_0 = data.loc[data.visitors == 0]   \n",
    "    q1 = np.percentile(data.visitors, 25, axis=0)\n",
    "    q3 = np.percentile(data.visitors, 75, axis=0)\n",
    "    k = 2.8\n",
    "    iqr = q3 - q1\n",
    "    df_temp = data.loc[data.visitors > q1 - k*iqr]\n",
    "    df_temp = data.loc[data.visitors < q3 + k*iqr]\n",
    "    frames = [df_0, df_temp]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "air[\"visit_data\"] = remove_outliers(air[\"visit_data\"])\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "df_test['air_store_id'] = df_test['id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "df_test['visit_date'] = df_test['id'].apply(lambda x: x.split('_')[-1])\n",
    "index_test = df_test['id']\n",
    "del df_test['id'], df_test['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hr Done!\n",
      "-----------------------------------------------------------------------------------------\n",
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hs Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mapping and dropping useless information in df_hr ...')\n",
    "s_1 = store_id_relation['air_store_id']\n",
    "s_2 = store_id_relation['hpg_store_id']\n",
    "a_h_map = dict(zip(s_2.values, s_1.values))\n",
    "del s_1, s_2\n",
    "\n",
    "hpg[\"reserve\"]['air_store_id'] = hpg[\"reserve\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"reserve\"] = hpg[\"reserve\"].drop('hpg_store_id', axis=1).dropna()\n",
    "\n",
    "\n",
    "print('mapping and dropping useless information in df_hr Done!')\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "\n",
    "hpg[\"store_info\"]['air_store_id'] = hpg[\"store_info\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"store_info\"] = hpg[\"store_info\"].drop('hpg_store_id', axis=1).dropna()\n",
    "print('mapping and dropping useless information in df_hs Done!')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking for correlations in data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "air[\"visit_data\"][\"num_of_week\"] = air[\"visit_data\"][\"visit_date\"].dt.dayofweek\n",
    "\n",
    "corr_table = air[\"visit_data\"]\n",
    "corr_table[\"visitors\"] = corr_table[\"visitors\"].map(np.log1p)\n",
    "corr_table = pd.merge(corr_table, air[\"store_info\"], on=\"air_store_id\")\n",
    "corr_table = pd.merge(corr_table, date_info,\n",
    "                      left_on=\"visit_date\", right_on=\"calendar_date\")\n",
    "corr_table[\"air_area_name\"] = corr_table[\"air_area_name\"].str.partition(\" \")\n",
    "corr_table[\"weekend_flg\"] = corr_table[\"num_of_week\"].map(lambda n: 1 if n in (1,2) else 0)\n",
    "corr_table[\"not_workday_flg\"] = corr_table[\"holiday_flg\"].combine(corr_table[\"weekend_flg\"], xor)\n",
    "corr_table = corr_table.drop(columns=[\"day_of_week\", \"visit_date\"])\n",
    "print(corr_table.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(corr_table.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_table[\"cumul_vis\"] = corr_table[\"visitors\"].cummax()\n",
    "corr_table.loc[:,[\"calendar_date\", \"cumul_vis\", \"visitors\"]].plot.line(x=\"calendar_date\", subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label encoding (?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(air[\"store_info\"]['air_genre_name'])\n",
    "air[\"store_info\"]['air_genre_name'] = le.fit_transform(air[\"store_info\"]['air_genre_name'])\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_area_name'])\n",
    "air[\"store_info\"]['air_area_name'] = le.fit_transform(air[\"store_info\"]['air_area_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "hpg[\"store_info\"]['hpg_genre_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_area_name'])\n",
    "hpg[\"store_info\"]['hpg_area_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_area_name'])\n",
    "\n",
    "\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_store_id'])\n",
    "\n",
    "\n",
    "air[\"reserve\"]['air_store_id'] = le.transform(air[\"reserve\"]['air_store_id'])\n",
    "air[\"store_info\"]['air_store_id'] = le.transform(air[\"store_info\"]['air_store_id'])\n",
    "air[\"visit_data\"]['air_store_id'] = le.transform(air[\"visit_data\"]['air_store_id'])\n",
    "hpg[\"reserve\"]['air_store_id'] = le.transform(hpg[\"reserve\"]['air_store_id'])\n",
    "hpg[\"store_info\"]['air_store_id'] = le.transform(hpg[\"store_info\"]['air_store_id'])\n",
    "\n",
    "df_test['air_store_id'] = le.transform(df_test['air_store_id'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seperating date time features done! ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re']= data_time.dt.year\n",
    "    data['Month_re'] = data_time.dt.month\n",
    "    data['DayOfYear_re'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re'] = data_time.dt.week\n",
    "    data['DayOfWeek_re'] = data_time.dt.dayofweek\n",
    "    data['Hour_re'] = data_time.dt.hour\n",
    "#    del data['reserve_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re_h']= data_time.dt.year\n",
    "    data['Month_re_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_h'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_h'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_h'] = data_time.dt.hour\n",
    "#    del data['reserve_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit']= data_time.dt.year\n",
    "    data['Month_re_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_visit'] = data_time.dt.day\n",
    "#    data['WeekOfYear_re_visit'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit'] = data_time.dt.hour\n",
    "#    del data['visit_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit_h']= data_time.dt.year\n",
    "    data['Month_re_visit_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit_h'] = data_time.dt.dayofyear\n",
    "    # data['DayOfMonth_re_visit_h'] = data_time.dt.day\n",
    "    data['WeekOfYear_re_visit_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit_h'] = data_time.dt.hour\n",
    "#    del data['visit_datetime']\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "print('seperating date time features done! ...')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging dataframes ...\n",
      "merging dataframes done!\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = [\n",
    "        'air_store_id__'\n",
    "        ]\n",
    "\n",
    "def merge_df(data, data_to_join):\n",
    "    # merge dataframes        \n",
    "    data = data.join(data_to_join, on='air_store_id', rsuffix='__', how='left')   \n",
    "    return data\n",
    "\n",
    "def fix_data(data):\n",
    "    # drop __ data    \n",
    "    for feature in features_to_drop:\n",
    "        del data[feature]\n",
    "    return data\n",
    "\n",
    "# Merge to df_train\n",
    "print('merging dataframes ...')\n",
    "df_train = merge_df(air[\"visit_data\"], air[\"reserve\"])\n",
    "df_train = merge_df(df_train, air[\"store_info\"])\n",
    "\n",
    "hpg[\"reserve\"]['reserve_visitors_hr'] = hpg[\"reserve\"]['reserve_visitors'] \n",
    "del hpg[\"reserve\"]['reserve_visitors'] \n",
    "\n",
    "hpg[\"store_info\"]['latitude_hr'] = hpg[\"store_info\"]['latitude'] \n",
    "del hpg[\"store_info\"]['latitude']\n",
    "\n",
    "hpg[\"store_info\"]['longitude_hr'] = hpg[\"store_info\"]['longitude'] \n",
    "del hpg[\"store_info\"]['longitude'] \n",
    "\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "gc.collect()\n",
    "fix_data(df_train)\n",
    "\n",
    "# Merge to df_test\n",
    "\n",
    "df_test = merge_df(df_test, air[\"reserve\"])\n",
    "df_test = merge_df(df_test, air[\"store_info\"])\n",
    "\n",
    "df_test = merge_df(df_test, hpg[\"store_info\"])\n",
    "df_test = merge_df(df_test, hpg[\"reserve\"])\n",
    "gc.collect()\n",
    "fix_data(df_test)\n",
    "\n",
    "\n",
    "print('merging dataframes done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop date-time-hour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'visit_datetime__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2524\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2525\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'visit_datetime__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-51984ea846cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reserve_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_datetime__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reserve_datetime__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_datetime_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdrop_datetime_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-51984ea846cf>\u001b[0m in \u001b[0;36mdrop_datetime_info\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdrop_datetime_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reserve_datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'visit_datetime__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reserve_datetime__'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrop_datetime_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__delitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2094\u001b[0m             \u001b[1;31m# there was no match, this call should raise the appropriate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2095\u001b[0m             \u001b[1;31m# exception:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m         \u001b[1;31m# delete from the caches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3900\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mselected\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnon\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3901\u001b[0m         \"\"\"\n\u001b[1;32m-> 3902\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3904\u001b[0m         \u001b[0mis_deleted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'visit_datetime__'"
     ]
    }
   ],
   "source": [
    "def drop_datetime_info(data):\n",
    "    del data['visit_date'], data['visit_datetime'], data['reserve_datetime'], data['visit_datetime__'], data['reserve_datetime__']\n",
    "    return data\n",
    "df_train = drop_datetime_info(df_train)\n",
    "\n",
    "def drop_datetime_info(data):\n",
    "    del data['visit_date'], data['visit_datetime'], data['reserve_datetime'], data['visit_datetime__'], data['reserve_datetime__']\n",
    "    return data\n",
    "df_test = drop_datetime_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.fillna(-1)\n",
    "test = df_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state=21)\n",
    "\n",
    "X_train, X_valid = train_test_split(train, test_size=0.05, random_state=43, shuffle=False)\n",
    "\n",
    "X = X_train.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_train[\"visitors\"].values)\n",
    "d_train = lgb.Dataset(X, y)\n",
    "\n",
    "X = X_valid.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_valid['visitors'].values)\n",
    "d_valid = lgb.Dataset(X, y)\n",
    "\n",
    "watchlist = [d_train, d_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[500]\ttraining's rmse: 0.669257\tvalid_1's rmse: 0.672579\n",
      "[1000]\ttraining's rmse: 0.63237\tvalid_1's rmse: 0.635897\n",
      "[1500]\ttraining's rmse: 0.615006\tvalid_1's rmse: 0.619115\n",
      "[2000]\ttraining's rmse: 0.607153\tvalid_1's rmse: 0.611714\n",
      "[2500]\ttraining's rmse: 0.602881\tvalid_1's rmse: 0.60761\n",
      "[3000]\ttraining's rmse: 0.600866\tvalid_1's rmse: 0.605749\n",
      "[3500]\ttraining's rmse: 0.599746\tvalid_1's rmse: 0.604711\n",
      "[4000]\ttraining's rmse: 0.599161\tvalid_1's rmse: 0.604151\n",
      "[4500]\ttraining's rmse: 0.598878\tvalid_1's rmse: 0.603872\n",
      "[5000]\ttraining's rmse: 0.598725\tvalid_1's rmse: 0.603679\n",
      "[5500]\ttraining's rmse: 0.598634\tvalid_1's rmse: 0.603552\n",
      "[6000]\ttraining's rmse: 0.598589\tvalid_1's rmse: 0.603476\n",
      "[6500]\ttraining's rmse: 0.598564\tvalid_1's rmse: 0.603402\n",
      "[7000]\ttraining's rmse: 0.598549\tvalid_1's rmse: 0.603354\n",
      "[7500]\ttraining's rmse: 0.59854\tvalid_1's rmse: 0.603325\n",
      "[8000]\ttraining's rmse: 0.598532\tvalid_1's rmse: 0.603302\n",
      "[8500]\ttraining's rmse: 0.598527\tvalid_1's rmse: 0.60327\n",
      "[9000]\ttraining's rmse: 0.598521\tvalid_1's rmse: 0.603243\n",
      "[9500]\ttraining's rmse: 0.598517\tvalid_1's rmse: 0.603225\n",
      "[10000]\ttraining's rmse: 0.598514\tvalid_1's rmse: 0.603212\n",
      "[10500]\ttraining's rmse: 0.598511\tvalid_1's rmse: 0.6032\n",
      "[11000]\ttraining's rmse: 0.598509\tvalid_1's rmse: 0.603191\n",
      "[11500]\ttraining's rmse: 0.598507\tvalid_1's rmse: 0.603185\n",
      "[12000]\ttraining's rmse: 0.598505\tvalid_1's rmse: 0.603178\n",
      "[12500]\ttraining's rmse: 0.598504\tvalid_1's rmse: 0.603174\n",
      "[13000]\ttraining's rmse: 0.598503\tvalid_1's rmse: 0.603168\n",
      "[13500]\ttraining's rmse: 0.598502\tvalid_1's rmse: 0.603164\n",
      "[14000]\ttraining's rmse: 0.598501\tvalid_1's rmse: 0.603161\n",
      "[14500]\ttraining's rmse: 0.5985\tvalid_1's rmse: 0.603158\n",
      "[15000]\ttraining's rmse: 0.5985\tvalid_1's rmse: 0.603156\n",
      "[15500]\ttraining's rmse: 0.5985\tvalid_1's rmse: 0.603154\n",
      "[16000]\ttraining's rmse: 0.598499\tvalid_1's rmse: 0.603152\n",
      "[16500]\ttraining's rmse: 0.598499\tvalid_1's rmse: 0.60315\n",
      "[17000]\ttraining's rmse: 0.598499\tvalid_1's rmse: 0.603149\n",
      "[17500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603148\n",
      "[18000]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603147\n",
      "[18500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603146\n",
      "[19000]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603145\n",
      "[19500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603144\n",
      "[20000]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603144\n",
      "[20500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603143\n",
      "[21000]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603142\n",
      "[21500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603142\n",
      "[22000]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603142\n",
      "[22500]\ttraining's rmse: 0.598498\tvalid_1's rmse: 0.603141\n",
      "[23000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603141\n",
      "[23500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603141\n",
      "[24000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.60314\n",
      "[24500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.60314\n",
      "[25000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.60314\n",
      "[25500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.60314\n",
      "[26000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[26500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[27000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[27500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[28000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[28500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603139\n",
      "[29000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[29500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[30000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[30500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[31000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[31500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[32000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[32500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[33000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[33500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[34000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[34500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[35000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[35500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[36000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[36500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[37000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[37500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[38000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[38500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[39000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[39500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[40000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[40500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[41000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[41500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603138\n",
      "[42000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[42500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[43000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[43500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[44000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[44500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[45000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[45500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[46000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[46500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[47000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[47500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[48000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[48500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[49000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[49500]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n",
      "[50000]\ttraining's rmse: 0.598497\tvalid_1's rmse: 0.603137\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields visit_date, visit_datetime, reserve_datetime, visit_datetime__, reserve_datetime__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-ac0c99a25902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mlgb_model1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtest_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mtest_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_probs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape, pred_parameter)\u001b[0m\n\u001b[0;32m   1788\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mnum_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1790\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_contrib\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_has_header\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_reshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1791\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1792\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_leaf_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtree_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleaf_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, data, num_iteration, raw_score, pred_leaf, pred_contrib, data_has_header, is_reshape)\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot use Dataset instance for prediction, please use raw data instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_data_from_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpandas_categorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m         \u001b[0mpredict_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mC_API_PREDICT_NORMAL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m', '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbad_fields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields visit_date, visit_datetime, reserve_datetime, visit_datetime__, reserve_datetime__"
     ]
    }
   ],
   "source": [
    "print(\"Training LGBM model...\")\n",
    "params = {\n",
    "    \"application\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 32,\n",
    "    \"min_sum_hessian_in_leaf\": 1e-2,\n",
    "    \"min_gain_to_split\": 0,\n",
    "    \n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"num_threads\": 4,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=35000, valid_sets=watchlist, verbose_eval=1000)\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "result.to_csv(\"LGB_sub.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
