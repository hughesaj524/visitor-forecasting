{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from operator import xor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_metrics import rmsle\n",
    "%matplotlib inline\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "air = {\n",
    "    \"reserve\": pd.read_csv(\"data/air/air_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/air/air_store_info.csv\"),\n",
    "    \"visit_data\": pd.read_csv(\"data/air/air_visit_data.csv\", parse_dates=[\"visit_date\"])\n",
    "}\n",
    "\n",
    "hpg = {\n",
    "    \"reserve\": pd.read_csv(\"data/hpg/hpg_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/hpg/hpg_store_info.csv\")\n",
    "}\n",
    "\n",
    "date_info = pd.read_csv(\"data/date_info.csv\", parse_dates=[\"calendar_date\"])\n",
    "store_id_relation = pd.read_csv(\"data/store_id_relation.csv\")\n",
    "\n",
    "def remove_outliers(data):\n",
    "    df_0 = data.loc[data.visitors == 0]   \n",
    "    q1 = np.percentile(data.visitors, 25, axis=0)\n",
    "    q3 = np.percentile(data.visitors, 75, axis=0)\n",
    "    k = 2.8\n",
    "    iqr = q3 - q1\n",
    "    df_temp = data.loc[data.visitors > q1 - k*iqr]\n",
    "    df_temp = data.loc[data.visitors < q3 + k*iqr]\n",
    "    frames = [df_0, df_temp]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "air[\"visit_data\"] = remove_outliers(air[\"visit_data\"])\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "df_test.head()\n",
    "df_test['air_store_id'] = df_test['id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "df_test['visit_date'] = df_test['id'].apply(lambda x: x.split('_')[-1])\n",
    "df_test.head()\n",
    "index_test = df_test['id']\n",
    "df_test.head()\n",
    "del df_test['id'], df_test['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hr Done!\n",
      "-----------------------------------------------------------------------------------------\n",
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hs Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mapping and dropping useless information in df_hr ...')\n",
    "s_1 = store_id_relation['air_store_id']\n",
    "s_2 = store_id_relation['hpg_store_id']\n",
    "a_h_map = dict(zip(s_2.values, s_1.values))\n",
    "del s_1, s_2\n",
    "\n",
    "hpg[\"reserve\"]['air_store_id'] = hpg[\"reserve\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"reserve\"] = hpg[\"reserve\"].drop('hpg_store_id', axis=1).dropna()\n",
    "\n",
    "\n",
    "print('mapping and dropping useless information in df_hr Done!')\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "\n",
    "hpg[\"store_info\"]['air_store_id'] = hpg[\"store_info\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"store_info\"] = hpg[\"store_info\"].drop('hpg_store_id', axis=1).dropna()\n",
    "print('mapping and dropping useless information in df_hs Done!')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking for correlations in data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           air_store_id  visitors  num_of_week air_genre_name air_area_name  \\\n",
      "0  air_ba937bf13d40fb24  3.258097            2     Dining bar      Tōkyō-to   \n",
      "1  air_25e9888d30b386df  3.091042            2        Izakaya      Tōkyō-to   \n",
      "2  air_fd6aac1043520e83  3.713572            2        Izakaya      Tōkyō-to   \n",
      "3  air_64d4491ad8cdb1c6  1.791759            2     Dining bar      Tōkyō-to   \n",
      "4  air_ee3a01f0c71a769f  2.944439            2    Cafe/Sweets  Shizuoka-ken   \n",
      "\n",
      "    latitude   longitude calendar_date  holiday_flg  weekend_flg  \\\n",
      "0  35.658068  139.751599    2016-01-13            0            1   \n",
      "1  35.626568  139.725858    2016-01-13            0            1   \n",
      "2  35.658068  139.751599    2016-01-13            0            1   \n",
      "3  35.658068  139.751599    2016-01-13            0            1   \n",
      "4  34.710895  137.725940    2016-01-13            0            1   \n",
      "\n",
      "   not_workday_flg  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n"
     ]
    }
   ],
   "source": [
    "air[\"visit_data\"][\"num_of_week\"] = air[\"visit_data\"][\"visit_date\"].dt.dayofweek\n",
    "\n",
    "corr_table = air[\"visit_data\"]\n",
    "corr_table[\"visitors\"] = corr_table[\"visitors\"].map(np.log1p)\n",
    "corr_table = pd.merge(corr_table, air[\"store_info\"], on=\"air_store_id\")\n",
    "corr_table = pd.merge(corr_table, date_info,\n",
    "                      left_on=\"visit_date\", right_on=\"calendar_date\")\n",
    "corr_table[\"air_area_name\"] = corr_table[\"air_area_name\"].str.partition(\" \")\n",
    "corr_table[\"weekend_flg\"] = corr_table[\"num_of_week\"].map(lambda n: 1 if n in (1,2) else 0)\n",
    "corr_table[\"not_workday_flg\"] = corr_table[\"holiday_flg\"].combine(corr_table[\"weekend_flg\"], xor)\n",
    "corr_table = corr_table.drop(columns=[\"day_of_week\", \"visit_date\"])\n",
    "print(corr_table.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 visitors  num_of_week  latitude  longitude  holiday_flg  \\\n",
      "visitors         1.000000     0.157647 -0.005448  -0.005496     0.026258   \n",
      "num_of_week      0.157647     1.000000  0.000692  -0.010221    -0.050779   \n",
      "latitude        -0.005448     0.000692  1.000000   0.652618     0.001884   \n",
      "longitude       -0.005496    -0.010221  0.652618   1.000000    -0.011837   \n",
      "holiday_flg      0.026258    -0.050779  0.001884  -0.011837     1.000000   \n",
      "weekend_flg     -0.081873    -0.500907  0.000979   0.012492    -0.091021   \n",
      "not_workday_flg -0.070737    -0.490379  0.001420   0.008290     0.274802   \n",
      "\n",
      "                 weekend_flg  not_workday_flg  \n",
      "visitors           -0.081873        -0.070737  \n",
      "num_of_week        -0.500907        -0.490379  \n",
      "latitude            0.000979         0.001420  \n",
      "longitude           0.012492         0.008290  \n",
      "holiday_flg        -0.091021         0.274802  \n",
      "weekend_flg         1.000000         0.886025  \n",
      "not_workday_flg     0.886025         1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(corr_table.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(air[\"store_info\"]['air_genre_name'])\n",
    "air[\"store_info\"]['air_genre_name'] = le.fit_transform(air[\"store_info\"]['air_genre_name'])\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_area_name'])\n",
    "air[\"store_info\"]['air_area_name'] = le.fit_transform(air[\"store_info\"]['air_area_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "hpg[\"store_info\"]['hpg_genre_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_area_name'])\n",
    "hpg[\"store_info\"]['hpg_area_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_area_name'])\n",
    "\n",
    "\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_store_id'])\n",
    "\n",
    "\n",
    "air[\"reserve\"]['air_store_id'] = le.transform(air[\"reserve\"]['air_store_id'])\n",
    "air[\"store_info\"]['air_store_id'] = le.transform(air[\"store_info\"]['air_store_id'])\n",
    "air[\"visit_data\"]['air_store_id'] = le.transform(air[\"visit_data\"]['air_store_id'])\n",
    "hpg[\"reserve\"]['air_store_id'] = le.transform(hpg[\"reserve\"]['air_store_id'])\n",
    "hpg[\"store_info\"]['air_store_id'] = le.transform(hpg[\"store_info\"]['air_store_id'])\n",
    "\n",
    "df_test['air_store_id'] = le.transform(df_test['air_store_id'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seperating date time features done! ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re']= data_time.dt.year\n",
    "    data['Month_re'] = data_time.dt.month\n",
    "    data['DayOfYear_re'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re'] = data_time.dt.dayofweek\n",
    "    data['Hour_re'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re_h']= data_time.dt.year\n",
    "    data['Month_re_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_h'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit']= data_time.dt.year\n",
    "    data['Month_re_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_visit'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit_h']= data_time.dt.year\n",
    "    data['Month_re_visit_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit_h'] = data_time.dt.dayofyear\n",
    "    data['WeekOfYear_re_visit_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "print('seperating date time features done! ...')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging dataframes ...\n",
      "merging dataframes done!\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = [\n",
    "        'air_store_id__'\n",
    "        ]\n",
    "\n",
    "def merge_df(data, data_to_join):\n",
    "    # merge dataframes        \n",
    "    data = data.join(data_to_join, on='air_store_id', rsuffix='__', how='left')   \n",
    "    return data\n",
    "\n",
    "def fix_data(data):\n",
    "    # drop __ data    \n",
    "    for feature in features_to_drop:\n",
    "        data.drop(feature, axis=1)\n",
    "    return data\n",
    "\n",
    "# Merge to df_train\n",
    "print('merging dataframes ...')\n",
    "df_train = merge_df(air[\"visit_data\"], air[\"reserve\"])\n",
    "df_train = merge_df(df_train, air[\"store_info\"])\n",
    "\n",
    "hpg[\"reserve\"]['reserve_visitors_hr'] = hpg[\"reserve\"]['reserve_visitors'] \n",
    "hpg[\"reserve\"].drop('reserve_visitors', axis=1) \n",
    "\n",
    "hpg[\"store_info\"]['latitude_hr'] = hpg[\"store_info\"]['latitude'] \n",
    "hpg[\"store_info\"].drop('latitude', axis=1)\n",
    "\n",
    "hpg[\"store_info\"]['longitude_hr'] = hpg[\"store_info\"]['longitude'] \n",
    "hpg[\"store_info\"].drop('longitude', axis=1) \n",
    "\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "gc.collect()\n",
    "fix_data(df_train)\n",
    "\n",
    "# Merge to df_test\n",
    "\n",
    "df_test = merge_df(df_test, air[\"reserve\"])\n",
    "df_test = merge_df(df_test, air[\"store_info\"])\n",
    "\n",
    "df_test = merge_df(df_test, hpg[\"store_info\"])\n",
    "df_test = merge_df(df_test, hpg[\"reserve\"])\n",
    "gc.collect()\n",
    "fix_data(df_test)\n",
    "\n",
    "\n",
    "print('merging dataframes done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop date-time-hour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_datetime_info(data):\n",
    "    data = data.drop(['visit_date', 'visit_datetime', 'reserve_datetime', 'visit_datetime__', 'reserve_datetime__'], axis=1)\n",
    "    return data\n",
    "df_train = drop_datetime_info(df_train)\n",
    "df_test = drop_datetime_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.fillna(-1)\n",
    "test = df_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state=524)\n",
    "\n",
    "X_pretrain, X_prevalid = train_test_split(train, test_size=0.05, random_state=1224, shuffle=False)\n",
    "\n",
    "X_train = X_pretrain.drop(['visitors'], axis=1)\n",
    "y_train = np.log1p(X_pretrain[\"visitors\"].values)\n",
    "\n",
    "X_valid = X_prevalid.drop(['visitors'], axis=1)\n",
    "y_valid = np.log1p(X_prevalid['visitors'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking mean rmse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mrmse = pd.DataFrame({\"id\": index_test, \"visitors\": pd.Series([train[\"visitors\"].mean()]*index_test.size)})\n",
    "mrmse.to_csv(\"MRMSE_sub.csv\", index=False) #0.88\n",
    "print(\"MRMSE written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test run without CV"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print(\"Training LGBM model...\")\n",
    "params = {\n",
    "    \"application\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"learning_rate\": 0.03,\n",
    "    \"num_leaves\": 32,\n",
    "    \"min_sum_hessian_in_leaf\": 1e-2,\n",
    "    \"min_gain_to_split\": 0,\n",
    "\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"num_threads\": 4,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "d_train = lgb.Dataset(X_train, y_train)\n",
    "d_valid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=35000, valid_sets=watchlist, verbose_eval=1000)\n",
    "\n",
    "print(\"Model trained. Predicting...\")\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "result.to_csv(\"LGB_sub.csv\", index=False) # 0.60\n",
    "print(\"Prediction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model with cross-validation...\n",
      "[1000]\ttraining's rmse: 0.741833\tvalid_1's rmse: 0.734585\n",
      "[2000]\ttraining's rmse: 0.706099\tvalid_1's rmse: 0.699121\n",
      "[3000]\ttraining's rmse: 0.682136\tvalid_1's rmse: 0.675538\n",
      "[4000]\ttraining's rmse: 0.664607\tvalid_1's rmse: 0.658298\n",
      "[5000]\ttraining's rmse: 0.651805\tvalid_1's rmse: 0.645935\n",
      "[6000]\ttraining's rmse: 0.64159\tvalid_1's rmse: 0.636245\n",
      "[7000]\ttraining's rmse: 0.633188\tvalid_1's rmse: 0.6282\n",
      "[8000]\ttraining's rmse: 0.626348\tvalid_1's rmse: 0.621683\n",
      "[9000]\ttraining's rmse: 0.620974\tvalid_1's rmse: 0.616687\n",
      "[10000]\ttraining's rmse: 0.61642\tvalid_1's rmse: 0.612475\n",
      "[11000]\ttraining's rmse: 0.612416\tvalid_1's rmse: 0.608808\n",
      "[12000]\ttraining's rmse: 0.609094\tvalid_1's rmse: 0.60577\n",
      "[13000]\ttraining's rmse: 0.60624\tvalid_1's rmse: 0.603276\n",
      "[14000]\ttraining's rmse: 0.604129\tvalid_1's rmse: 0.601476\n",
      "[15000]\ttraining's rmse: 0.602293\tvalid_1's rmse: 0.599785\n",
      "[16000]\ttraining's rmse: 0.600787\tvalid_1's rmse: 0.59842\n",
      "[17000]\ttraining's rmse: 0.599614\tvalid_1's rmse: 0.597399\n",
      "[18000]\ttraining's rmse: 0.598609\tvalid_1's rmse: 0.596511\n",
      "[19000]\ttraining's rmse: 0.597837\tvalid_1's rmse: 0.595859\n",
      "[20000]\ttraining's rmse: 0.597171\tvalid_1's rmse: 0.595348\n",
      "[21000]\ttraining's rmse: 0.596612\tvalid_1's rmse: 0.594925\n",
      "[22000]\ttraining's rmse: 0.596143\tvalid_1's rmse: 0.594557\n",
      "[23000]\ttraining's rmse: 0.595743\tvalid_1's rmse: 0.594254\n",
      "[24000]\ttraining's rmse: 0.595439\tvalid_1's rmse: 0.594028\n",
      "[25000]\ttraining's rmse: 0.595176\tvalid_1's rmse: 0.593833\n",
      "[26000]\ttraining's rmse: 0.594962\tvalid_1's rmse: 0.593671\n",
      "[27000]\ttraining's rmse: 0.594791\tvalid_1's rmse: 0.593562\n",
      "[28000]\ttraining's rmse: 0.594644\tvalid_1's rmse: 0.593479\n",
      "[29000]\ttraining's rmse: 0.594515\tvalid_1's rmse: 0.593414\n",
      "[30000]\ttraining's rmse: 0.594401\tvalid_1's rmse: 0.593346\n",
      "[31000]\ttraining's rmse: 0.59431\tvalid_1's rmse: 0.593297\n",
      "[32000]\ttraining's rmse: 0.594231\tvalid_1's rmse: 0.593255\n",
      "[33000]\ttraining's rmse: 0.594164\tvalid_1's rmse: 0.593233\n",
      "[34000]\ttraining's rmse: 0.594106\tvalid_1's rmse: 0.593216\n",
      "[35000]\ttraining's rmse: 0.594055\tvalid_1's rmse: 0.593216\n",
      "[36000]\ttraining's rmse: 0.594012\tvalid_1's rmse: 0.593211\n",
      "[37000]\ttraining's rmse: 0.593975\tvalid_1's rmse: 0.593211\n",
      "Model trained. Predicting...\n",
      "Prediction complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LGBM model with cross-validation...\")\n",
    "params = {\n",
    "    \"application\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_leaves\": 32,\n",
    "    \"min_sum_hessian_in_leaf\": 1e-2,\n",
    "    \"min_gain_to_split\": 0,\n",
    "    \n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"num_threads\": 4,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "d_train = lgb.Dataset(X_train, y_train)\n",
    "d_valid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "# default value of folds: 5\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=37000, valid_sets=watchlist, verbose_eval=1000)\n",
    "\n",
    "print(\"Model trained. Predicting...\")\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "result.to_csv(\"LGB_sub.csv\", index=False) # 0.60\n",
    "print(\"Prediction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
