{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from operator import xor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_metrics import rmsle\n",
    "%matplotlib inline\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "air = {\n",
    "    \"reserve\": pd.read_csv(\"data/air/air_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/air/air_store_info.csv\"),\n",
    "    \"visit_data\": pd.read_csv(\"data/air/air_visit_data.csv\", parse_dates=[\"visit_date\"])\n",
    "}\n",
    "\n",
    "hpg = {\n",
    "    \"reserve\": pd.read_csv(\"data/hpg/hpg_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/hpg/hpg_store_info.csv\")\n",
    "}\n",
    "\n",
    "date_info = pd.read_csv(\"data/date_info.csv\", parse_dates=[\"calendar_date\"])\n",
    "store_id_relation = pd.read_csv(\"data/store_id_relation.csv\")\n",
    "\n",
    "def remove_outliers(data):\n",
    "    df_0 = data.loc[data.visitors == 0]   \n",
    "    q1 = np.percentile(data.visitors, 25, axis=0)\n",
    "    q3 = np.percentile(data.visitors, 75, axis=0)\n",
    "    k = 2.8\n",
    "    iqr = q3 - q1\n",
    "    df_temp = data.loc[data.visitors > q1 - k*iqr]\n",
    "    df_temp = data.loc[data.visitors < q3 + k*iqr]\n",
    "    frames = [df_0, df_temp]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "air[\"visit_data\"] = remove_outliers(air[\"visit_data\"])\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "df_test['air_store_id'] = df_test['id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "df_test['visit_date'] = df_test['id'].apply(lambda x: x.split('_')[-1])\n",
    "index_test = df_test['id']\n",
    "del df_test['id'], df_test['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hr Done!\n",
      "-----------------------------------------------------------------------------------------\n",
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hs Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mapping and dropping useless information in df_hr ...')\n",
    "s_1 = store_id_relation['air_store_id']\n",
    "s_2 = store_id_relation['hpg_store_id']\n",
    "a_h_map = dict(zip(s_2.values, s_1.values))\n",
    "del s_1, s_2\n",
    "\n",
    "hpg[\"reserve\"]['air_store_id'] = hpg[\"reserve\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"reserve\"] = hpg[\"reserve\"].drop('hpg_store_id', axis=1).dropna()\n",
    "\n",
    "\n",
    "print('mapping and dropping useless information in df_hr Done!')\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "\n",
    "hpg[\"store_info\"]['air_store_id'] = hpg[\"store_info\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"store_info\"] = hpg[\"store_info\"].drop('hpg_store_id', axis=1).dropna()\n",
    "print('mapping and dropping useless information in df_hs Done!')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking for correlations in data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "air[\"visit_data\"][\"num_of_week\"] = air[\"visit_data\"][\"visit_date\"].dt.dayofweek\n",
    "\n",
    "corr_table = air[\"visit_data\"]\n",
    "corr_table[\"visitors\"] = corr_table[\"visitors\"].map(np.log1p)\n",
    "corr_table = pd.merge(corr_table, air[\"store_info\"], on=\"air_store_id\")\n",
    "corr_table = pd.merge(corr_table, date_info,\n",
    "                      left_on=\"visit_date\", right_on=\"calendar_date\")\n",
    "corr_table[\"air_area_name\"] = corr_table[\"air_area_name\"].str.partition(\" \")\n",
    "corr_table[\"weekend_flg\"] = corr_table[\"num_of_week\"].map(lambda n: 1 if n in (1,2) else 0)\n",
    "corr_table[\"not_workday_flg\"] = corr_table[\"holiday_flg\"].combine(corr_table[\"weekend_flg\"], xor)\n",
    "corr_table = corr_table.drop(columns=[\"day_of_week\", \"visit_date\"])\n",
    "print(corr_table.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(corr_table.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_table[\"cumul_vis\"] = corr_table[\"visitors\"].cummax()\n",
    "corr_table.loc[:,[\"calendar_date\", \"cumul_vis\", \"visitors\"]].plot.line(x=\"calendar_date\", subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label encoding (?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(air[\"store_info\"]['air_genre_name'])\n",
    "air[\"store_info\"]['air_genre_name'] = le.fit_transform(air[\"store_info\"]['air_genre_name'])\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_area_name'])\n",
    "air[\"store_info\"]['air_area_name'] = le.fit_transform(air[\"store_info\"]['air_area_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "hpg[\"store_info\"]['hpg_genre_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_area_name'])\n",
    "hpg[\"store_info\"]['hpg_area_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_area_name'])\n",
    "\n",
    "\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_store_id'])\n",
    "\n",
    "\n",
    "air[\"reserve\"]['air_store_id'] = le.transform(air[\"reserve\"]['air_store_id'])\n",
    "air[\"store_info\"]['air_store_id'] = le.transform(air[\"store_info\"]['air_store_id'])\n",
    "air[\"visit_data\"]['air_store_id'] = le.transform(air[\"visit_data\"]['air_store_id'])\n",
    "hpg[\"reserve\"]['air_store_id'] = le.transform(hpg[\"reserve\"]['air_store_id'])\n",
    "hpg[\"store_info\"]['air_store_id'] = le.transform(hpg[\"store_info\"]['air_store_id'])\n",
    "\n",
    "df_test['air_store_id'] = le.transform(df_test['air_store_id'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seperating date time features done! ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re']= data_time.dt.year\n",
    "    data['Month_re'] = data_time.dt.month\n",
    "    data['DayOfYear_re'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re'] = data_time.dt.dayofweek\n",
    "    data['Hour_re'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re_h']= data_time.dt.year\n",
    "    data['Month_re_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_h'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit']= data_time.dt.year\n",
    "    data['Month_re_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_visit'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit_h']= data_time.dt.year\n",
    "    data['Month_re_visit_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit_h'] = data_time.dt.dayofyear\n",
    "    data['WeekOfYear_re_visit_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "print('seperating date time features done! ...')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visit_datetime', 'reserve_datetime', 'reserve_visitors',\n",
       "       'air_store_id', 'Year_re_h', 'Month_re_h', 'DayOfYear_re_h',\n",
       "       'DayOfWeek_re_h', 'Hour_re_h', 'Year_re_visit_h', 'Month_re_visit_h',\n",
       "       'DayOfYear_re_visit_h', 'WeekOfYear_re_visit_h', 'DayOfWeek_re_visit_h',\n",
       "       'Hour_re_visit_h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpg[\"reserve\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging dataframes ...\n",
      "merging dataframes done!\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = [\n",
    "        'air_store_id__'\n",
    "        ]\n",
    "\n",
    "def merge_df(data, data_to_join):\n",
    "    # merge dataframes        \n",
    "    data = data.join(data_to_join, on='air_store_id', rsuffix='__', how='left')   \n",
    "    return data\n",
    "\n",
    "def fix_data(data):\n",
    "    # drop __ data    \n",
    "    for feature in features_to_drop:\n",
    "        data.drop(feature, axis=1)\n",
    "    return data\n",
    "\n",
    "# Merge to df_train\n",
    "print('merging dataframes ...')\n",
    "df_train = merge_df(air[\"visit_data\"], air[\"reserve\"])\n",
    "df_train = merge_df(df_train, air[\"store_info\"])\n",
    "\n",
    "hpg[\"reserve\"]['reserve_visitors_hr'] = hpg[\"reserve\"]['reserve_visitors'] \n",
    "hpg[\"reserve\"].drop('reserve_visitors', axis=1) \n",
    "\n",
    "hpg[\"store_info\"]['latitude_hr'] = hpg[\"store_info\"]['latitude'] \n",
    "hpg[\"store_info\"].drop('latitude', axis=1)\n",
    "\n",
    "hpg[\"store_info\"]['longitude_hr'] = hpg[\"store_info\"]['longitude'] \n",
    "hpg[\"store_info\"].drop('longitude', axis=1) \n",
    "\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "gc.collect()\n",
    "fix_data(df_train)\n",
    "\n",
    "# Merge to df_test\n",
    "\n",
    "df_test = merge_df(df_test, air[\"reserve\"])\n",
    "df_test = merge_df(df_test, air[\"store_info\"])\n",
    "\n",
    "df_test = merge_df(df_test, hpg[\"store_info\"])\n",
    "df_test = merge_df(df_test, hpg[\"reserve\"])\n",
    "gc.collect()\n",
    "fix_data(df_test)\n",
    "\n",
    "\n",
    "print('merging dataframes done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop date-time-hour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_datetime_info(data):\n",
    "    data = data.drop(['visit_date', 'visit_datetime', 'reserve_datetime', 'visit_datetime__', 'reserve_datetime__'], axis=1)\n",
    "    return data\n",
    "df_train = drop_datetime_info(df_train)\n",
    "df_test = drop_datetime_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_store_id', 'air_store_id__', 'reserve_visitors', 'Year_re',\n",
       "       'Month_re', 'DayOfYear_re', 'DayOfWeek_re', 'Hour_re', 'Year_re_visit',\n",
       "       'Month_re_visit', 'DayOfYear_re_visit', 'DayOfWeek_re_visit',\n",
       "       'Hour_re_visit', 'air_store_id__', 'air_genre_name', 'air_area_name',\n",
       "       'latitude', 'longitude', 'hpg_genre_name', 'hpg_area_name',\n",
       "       'latitude__', 'longitude__', 'air_store_id__', 'latitude_hr',\n",
       "       'longitude_hr', 'reserve_visitors__', 'air_store_id__', 'Year_re_h',\n",
       "       'Month_re_h', 'DayOfYear_re_h', 'DayOfWeek_re_h', 'Hour_re_h',\n",
       "       'Year_re_visit_h', 'Month_re_visit_h', 'DayOfYear_re_visit_h',\n",
       "       'WeekOfYear_re_visit_h', 'DayOfWeek_re_visit_h', 'Hour_re_visit_h',\n",
       "       'reserve_visitors_hr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.fillna(-1)\n",
    "test = df_test.fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state=21)\n",
    "\n",
    "X_train, X_valid = train_test_split(train, test_size=0.05, random_state=43, shuffle=False)\n",
    "\n",
    "X = X_train.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_train[\"visitors\"].values)\n",
    "d_train = lgb.Dataset(X, y)\n",
    "\n",
    "X = X_valid.drop(['visitors'], axis=1)\n",
    "y = np.log1p(X_valid['visitors'].values)\n",
    "d_valid = lgb.Dataset(X, y)\n",
    "\n",
    "watchlist = [d_train, d_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.38629436 3.33220451 2.63905733 ... 2.19722458 2.99573227 3.66356165]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_Dataset__init_from_csc',\n",
       " '_Dataset__init_from_csr',\n",
       " '_Dataset__init_from_np2d',\n",
       " '__class__',\n",
       " '__del__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_free_handle',\n",
       " '_lazy_init',\n",
       " '_predictor',\n",
       " '_reverse_update_params',\n",
       " '_set_predictor',\n",
       " '_update_params',\n",
       " 'categorical_feature',\n",
       " 'construct',\n",
       " 'create_valid',\n",
       " 'data',\n",
       " 'data_has_header',\n",
       " 'feature_name',\n",
       " 'free_raw_data',\n",
       " 'get_field',\n",
       " 'get_group',\n",
       " 'get_init_score',\n",
       " 'get_label',\n",
       " 'get_ref_chain',\n",
       " 'get_weight',\n",
       " 'group',\n",
       " 'handle',\n",
       " 'init_score',\n",
       " 'label',\n",
       " 'num_data',\n",
       " 'num_feature',\n",
       " 'pandas_categorical',\n",
       " 'params',\n",
       " 'params_back_up',\n",
       " 'predictor',\n",
       " 'reference',\n",
       " 'save_binary',\n",
       " 'set_categorical_feature',\n",
       " 'set_feature_name',\n",
       " 'set_field',\n",
       " 'set_group',\n",
       " 'set_init_score',\n",
       " 'set_label',\n",
       " 'set_reference',\n",
       " 'set_weight',\n",
       " 'silent',\n",
       " 'subset',\n",
       " 'used_indices',\n",
       " 'weight']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(d_train.label)\n",
    "dir(d_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-9cb1da526d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mlgb_model1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0md_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m35000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwatchlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mtest_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb_model1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    199\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1522\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1523\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training LGBM model...\")\n",
    "params = {\n",
    "    \"application\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 32,\n",
    "    \"min_sum_hessian_in_leaf\": 1e-2,\n",
    "    \"min_gain_to_split\": 0,\n",
    "    \n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"num_threads\": 4,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=35000, valid_sets=watchlist, verbose_eval=1000)\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "result.to_csv(\"LGB_sub.csv\", index=False) # 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Set up and fit Bayesian Ridge Regression\n",
    "bayesian_ridge = BayesianRidge(compute_score=True)\n",
    "n_folds = 5\n",
    "alpha_1s = np.logspace(-6,-0.5,5)\n",
    "alpha_2s = np.logspace(-6,-0.5,5)\n",
    "lambda_1s = np.logspace(-6,-0.5,5)\n",
    "lambda_2s = np.logspace(-6,-0.5,5)\n",
    "tuned_parameters = [{'alpha_1': alpha_1s,\n",
    "                   'alpha_2': alpha_2s,\n",
    "                   'lambda_1': lambda_1s,\n",
    "                   'lambda_2': lambda_2s}]\n",
    "clf_bayesian_ridge = GridSearchCV(bayesian_ridge, tuned_parameters, cv=n_folds)\n",
    "clf_bayesian_ridge.fit(X_train, y_train)\n",
    "# Get the raw best predictor so that we can set return_std=True\n",
    "clf_bayesian_ridge_best = clf_bayesian_ridge.best_estimator_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(clf_bayesian_ridge_best)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
