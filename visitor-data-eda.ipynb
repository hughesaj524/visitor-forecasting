{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from operator import xor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_metrics import rmsle\n",
    "%matplotlib inline\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "air = {\n",
    "    \"reserve\": pd.read_csv(\"data/air/air_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/air/air_store_info.csv\"),\n",
    "    \"visit_data\": pd.read_csv(\"data/air/air_visit_data.csv\", parse_dates=[\"visit_date\"])\n",
    "}\n",
    "\n",
    "hpg = {\n",
    "    \"reserve\": pd.read_csv(\"data/hpg/hpg_reserve.csv\", parse_dates=[\"visit_datetime\", \"reserve_datetime\"]),\n",
    "    \"store_info\": pd.read_csv(\"data/hpg/hpg_store_info.csv\")\n",
    "}\n",
    "\n",
    "date_info = pd.read_csv(\"data/date_info.csv\", parse_dates=[\"calendar_date\"])\n",
    "store_id_relation = pd.read_csv(\"data/store_id_relation.csv\")\n",
    "\n",
    "def remove_outliers(data):\n",
    "    df_0 = data.loc[data.visitors == 0]   \n",
    "    q1 = np.percentile(data.visitors, 25, axis=0)\n",
    "    q3 = np.percentile(data.visitors, 75, axis=0)\n",
    "    k = 2.8\n",
    "    iqr = q3 - q1\n",
    "    df_temp = data.loc[data.visitors > q1 - k*iqr]\n",
    "    df_temp = data.loc[data.visitors < q3 + k*iqr]\n",
    "    frames = [df_0, df_temp]\n",
    "    result = pd.concat(frames)\n",
    "    return result\n",
    "\n",
    "air[\"visit_data\"] = remove_outliers(air[\"visit_data\"])\n",
    "\n",
    "df_test = pd.read_csv('sample_submission.csv')\n",
    "df_test['air_store_id'] = df_test['id'].apply(lambda x: '_'.join(x.split('_')[:2]))\n",
    "df_test['visit_date'] = df_test['id'].apply(lambda x: x.split('_')[-1])\n",
    "index_test = df_test['id']\n",
    "del df_test['id'], df_test['visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hr Done!\n",
      "-----------------------------------------------------------------------------------------\n",
      "mapping and dropping useless information in df_hr ...\n",
      "mapping and dropping useless information in df_hs Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "468"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('mapping and dropping useless information in df_hr ...')\n",
    "s_1 = store_id_relation['air_store_id']\n",
    "s_2 = store_id_relation['hpg_store_id']\n",
    "a_h_map = dict(zip(s_2.values, s_1.values))\n",
    "del s_1, s_2\n",
    "\n",
    "hpg[\"reserve\"]['air_store_id'] = hpg[\"reserve\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"reserve\"] = hpg[\"reserve\"].drop('hpg_store_id', axis=1).dropna()\n",
    "\n",
    "\n",
    "print('mapping and dropping useless information in df_hr Done!')\n",
    "print(\"-----------------------------------------------------------------------------------------\")\n",
    "\n",
    "print('mapping and dropping useless information in df_hr ...')\n",
    "\n",
    "hpg[\"store_info\"]['air_store_id'] = hpg[\"store_info\"]['hpg_store_id'].map(a_h_map)\n",
    "hpg[\"store_info\"] = hpg[\"store_info\"].drop('hpg_store_id', axis=1).dropna()\n",
    "print('mapping and dropping useless information in df_hs Done!')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking for correlations in data:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "air[\"visit_data\"][\"num_of_week\"] = air[\"visit_data\"][\"visit_date\"].dt.dayofweek\n",
    "\n",
    "corr_table = air[\"visit_data\"]\n",
    "corr_table[\"visitors\"] = corr_table[\"visitors\"].map(np.log1p)\n",
    "corr_table = pd.merge(corr_table, air[\"store_info\"], on=\"air_store_id\")\n",
    "corr_table = pd.merge(corr_table, date_info,\n",
    "                      left_on=\"visit_date\", right_on=\"calendar_date\")\n",
    "corr_table[\"air_area_name\"] = corr_table[\"air_area_name\"].str.partition(\" \")\n",
    "corr_table[\"weekend_flg\"] = corr_table[\"num_of_week\"].map(lambda n: 1 if n in (1,2) else 0)\n",
    "corr_table[\"not_workday_flg\"] = corr_table[\"holiday_flg\"].combine(corr_table[\"weekend_flg\"], xor)\n",
    "corr_table = corr_table.drop(columns=[\"day_of_week\", \"visit_date\"])\n",
    "print(corr_table.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(corr_table.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corr_table[\"cumul_vis\"] = corr_table[\"visitors\"].cummax()\n",
    "corr_table.loc[:,[\"calendar_date\", \"cumul_vis\", \"visitors\"]].plot.line(x=\"calendar_date\", subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label encoding (?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.fit(air[\"store_info\"]['air_genre_name'])\n",
    "air[\"store_info\"]['air_genre_name'] = le.fit_transform(air[\"store_info\"]['air_genre_name'])\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_area_name'])\n",
    "air[\"store_info\"]['air_area_name'] = le.fit_transform(air[\"store_info\"]['air_area_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "hpg[\"store_info\"]['hpg_genre_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_genre_name'])\n",
    "\n",
    "le.fit(hpg[\"store_info\"]['hpg_area_name'])\n",
    "hpg[\"store_info\"]['hpg_area_name'] = le.fit_transform(hpg[\"store_info\"]['hpg_area_name'])\n",
    "\n",
    "\n",
    "\n",
    "le.fit(air[\"store_info\"]['air_store_id'])\n",
    "\n",
    "\n",
    "air[\"reserve\"]['air_store_id'] = le.transform(air[\"reserve\"]['air_store_id'])\n",
    "air[\"store_info\"]['air_store_id'] = le.transform(air[\"store_info\"]['air_store_id'])\n",
    "air[\"visit_data\"]['air_store_id'] = le.transform(air[\"visit_data\"]['air_store_id'])\n",
    "hpg[\"reserve\"]['air_store_id'] = le.transform(hpg[\"reserve\"]['air_store_id'])\n",
    "hpg[\"store_info\"]['air_store_id'] = le.transform(hpg[\"store_info\"]['air_store_id'])\n",
    "\n",
    "df_test['air_store_id'] = le.transform(df_test['air_store_id'])\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seperating date time features done! ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re']= data_time.dt.year\n",
    "    data['Month_re'] = data_time.dt.month\n",
    "    data['DayOfYear_re'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re'] = data_time.dt.dayofweek\n",
    "    data['Hour_re'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reservation datetime\n",
    "    data_time = pd.to_datetime(data.reserve_datetime, format=time_format)\n",
    "    data['Year_re_h']= data_time.dt.year\n",
    "    data['Month_re_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_h'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "\n",
    "time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit']= data_time.dt.year\n",
    "    data['Month_re_visit'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit'] = data_time.dt.dayofyear\n",
    "    data['DayOfWeek_re_visit'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(air[\"reserve\"])\n",
    "\n",
    "\n",
    "def seperate_date(data):\n",
    "    # split date feature in reserved visiting datetime\n",
    "    data_time = pd.to_datetime(data.visit_datetime, format=time_format)\n",
    "    data['Year_re_visit_h']= data_time.dt.year\n",
    "    data['Month_re_visit_h'] = data_time.dt.month\n",
    "    data['DayOfYear_re_visit_h'] = data_time.dt.dayofyear\n",
    "    data['WeekOfYear_re_visit_h'] = data_time.dt.week\n",
    "    data['DayOfWeek_re_visit_h'] = data_time.dt.dayofweek\n",
    "    data['Hour_re_visit_h'] = data_time.dt.hour\n",
    "    return data\n",
    "\n",
    "seperate_date(hpg[\"reserve\"])\n",
    "\n",
    "print('seperating date time features done! ...')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['visit_datetime', 'reserve_datetime', 'reserve_visitors',\n",
       "       'air_store_id', 'Year_re_h', 'Month_re_h', 'DayOfYear_re_h',\n",
       "       'DayOfWeek_re_h', 'Hour_re_h', 'Year_re_visit_h', 'Month_re_visit_h',\n",
       "       'DayOfYear_re_visit_h', 'WeekOfYear_re_visit_h', 'DayOfWeek_re_visit_h',\n",
       "       'Hour_re_visit_h'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hpg[\"reserve\"].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging dataframes ...\n",
      "merging dataframes done!\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "features_to_drop = [\n",
    "        'air_store_id__'\n",
    "        ]\n",
    "\n",
    "def merge_df(data, data_to_join):\n",
    "    # merge dataframes        \n",
    "    data = data.join(data_to_join, on='air_store_id', rsuffix='__', how='left')   \n",
    "    return data\n",
    "\n",
    "def fix_data(data):\n",
    "    # drop __ data    \n",
    "    for feature in features_to_drop:\n",
    "        data.drop(feature, axis=1)\n",
    "    return data\n",
    "\n",
    "# Merge to df_train\n",
    "print('merging dataframes ...')\n",
    "df_train = merge_df(air[\"visit_data\"], air[\"reserve\"])\n",
    "df_train = merge_df(df_train, air[\"store_info\"])\n",
    "\n",
    "hpg[\"reserve\"]['reserve_visitors_hr'] = hpg[\"reserve\"]['reserve_visitors'] \n",
    "hpg[\"reserve\"].drop('reserve_visitors', axis=1) \n",
    "\n",
    "hpg[\"store_info\"]['latitude_hr'] = hpg[\"store_info\"]['latitude'] \n",
    "hpg[\"store_info\"].drop('latitude', axis=1)\n",
    "\n",
    "hpg[\"store_info\"]['longitude_hr'] = hpg[\"store_info\"]['longitude'] \n",
    "hpg[\"store_info\"].drop('longitude', axis=1) \n",
    "\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "df_train = merge_df(df_train, hpg[\"store_info\"])\n",
    "gc.collect()\n",
    "fix_data(df_train)\n",
    "\n",
    "# Merge to df_test\n",
    "\n",
    "df_test = merge_df(df_test, air[\"reserve\"])\n",
    "df_test = merge_df(df_test, air[\"store_info\"])\n",
    "\n",
    "df_test = merge_df(df_test, hpg[\"store_info\"])\n",
    "df_test = merge_df(df_test, hpg[\"reserve\"])\n",
    "gc.collect()\n",
    "fix_data(df_test)\n",
    "\n",
    "\n",
    "print('merging dataframes done!')\n",
    "gc.collect()\n",
    "print(\"=========================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop date-time-hour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_datetime_info(data):\n",
    "    data = data.drop(['visit_date', 'visit_datetime', 'reserve_datetime', 'visit_datetime__', 'reserve_datetime__'], axis=1)\n",
    "    return data\n",
    "df_train = drop_datetime_info(df_train)\n",
    "df_test = drop_datetime_info(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.fillna(-1).iloc[:1000000]\n",
    "test = df_test.fillna(-1).iloc[:1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train, random_state=524)\n",
    "\n",
    "X_pretrain, X_prevalid = train_test_split(train, test_size=0.05, random_state=524, shuffle=False)\n",
    "\n",
    "X_train = X_pretrain.drop(['visitors'], axis=1)\n",
    "y_train = np.log1p(X_pretrain[\"visitors\"].values)\n",
    "\n",
    "X_valid = X_prevalid.drop(['visitors'], axis=1)\n",
    "y_valid = np.log1p(X_prevalid['visitors'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking mean rmse"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mrmse = pd.DataFrame({\"id\": index_test, \"visitors\": pd.Series([train[\"visitors\"].mean()]*index_test.size)})\n",
    "mrmse.to_csv(\"MRMSE_sub.csv\", index=False) #0.88\n",
    "print(\"MRMSE written!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LGBM model...\n",
      "[1000]\ttraining's rmse: 0.615307\tvalid_1's rmse: 0.611369\n",
      "[2000]\ttraining's rmse: 0.597215\tvalid_1's rmse: 0.595273\n",
      "[3000]\ttraining's rmse: 0.594414\tvalid_1's rmse: 0.593297\n",
      "[4000]\ttraining's rmse: 0.593902\tvalid_1's rmse: 0.593159\n",
      "[5000]\ttraining's rmse: 0.593798\tvalid_1's rmse: 0.593234\n",
      "[6000]\ttraining's rmse: 0.593767\tvalid_1's rmse: 0.593275\n",
      "[7000]\ttraining's rmse: 0.593753\tvalid_1's rmse: 0.593293\n",
      "[8000]\ttraining's rmse: 0.593746\tvalid_1's rmse: 0.593304\n",
      "[9000]\ttraining's rmse: 0.593742\tvalid_1's rmse: 0.59331\n",
      "[10000]\ttraining's rmse: 0.59374\tvalid_1's rmse: 0.593312\n",
      "[11000]\ttraining's rmse: 0.593738\tvalid_1's rmse: 0.593315\n",
      "[12000]\ttraining's rmse: 0.593737\tvalid_1's rmse: 0.593317\n",
      "[13000]\ttraining's rmse: 0.593737\tvalid_1's rmse: 0.593318\n",
      "[14000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593319\n",
      "[15000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.59332\n",
      "[16000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593321\n",
      "[17000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593321\n",
      "[18000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593321\n",
      "[19000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593321\n",
      "[20000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593322\n",
      "[21000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593322\n",
      "[22000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593322\n",
      "[23000]\ttraining's rmse: 0.593736\tvalid_1's rmse: 0.593322\n"
     ]
    }
   ],
   "source": [
    "print(\"Training LGBM model...\")\n",
    "params = {\n",
    "    \"application\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"num_leaves\": 32,\n",
    "    \"min_sum_hessian_in_leaf\": 1e-2,\n",
    "    \"min_gain_to_split\": 0,\n",
    "    \n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"num_threads\": 4,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "d_train = lgb.Dataset(X_train, y_train)\n",
    "d_valid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "watchlist = [d_train, d_valid]\n",
    "\n",
    "lgb_model1 = lgb.train(params, train_set=d_train, num_boost_round=35000, valid_sets=watchlist, verbose_eval=1000)\n",
    "\n",
    "print(\"Model trained. Predicting...\")\n",
    "\n",
    "test_probs = lgb_model1.predict(test)\n",
    "test_probs = np.expm1(test_probs)\n",
    "\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": test_probs})\n",
    "result.to_csv(\"LGB_sub.csv\", index=False) # 0.60\n",
    "print(\"Prediction complete.\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Set up and fit Random Forest Regression\n",
    "rf_regressor = RandomForestRegressor(random_state=56, warm_start=False, verbose=10, n_jobs=-1)\n",
    "print(\"Starting training...\")\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Starting prediction...\")\n",
    "test = test.drop([\"reserve_visitors__\", \"Year_re_h\", \"Month_re_h\", \"DayOfYear_re_h\", \"DayOfWeek_re_h\", \"Hour_re_h\", \"Year_re_visit_h\", \"Month_re_visit_h\", \"DayOfWeek_re_visit_h\", \"Hour_re_visit_h\", \"resere_visitors_hr\", \"DayOfYear_re_visit_h\"], axis=1)\n",
    "rf_predict = np.expm1(rf_regressor.predict(test))\n",
    "result = pd.DataFrame({\"id\": index_test, \"visitors\": rf_predict})\n",
    "result.to_csv(\"RF_sub.csv\", index=False) # 0.00\n",
    "print(\"Prediction complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
